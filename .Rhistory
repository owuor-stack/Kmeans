data$LEFTOVER = as.numeric(data$LEFTOVER)
data$HOUSE= as.numeric(data$HOUSE)
data$HANDSET_PRICE = as.numeric(data$HANDSET_PRICE)
data$OVER_15MINS_CALLS_PER_MONTH = as.numeric(data$OVER_15MINS_CALLS_PER_MONTH)
data$AVERAGE_CALL_DURATION = as.numeric(data$AVERAGE_CALL_DURATION)
data$REPORTED_SATISFACTION = as.numeric(data$REPORTED_SATISFACTION)
data$REPORTED_USAGE_LEVEL = as.numeric(data$REPORTED_USAGE_LEVEL)
data$CONSIDERING_CHANGE_OF_PLAN = as.numeric(data$CONSIDERING_CHANGE_OF_PLAN)
data$LEAVE = data$LEAVE
set.seed(1234)
ind= sample(2, nrow(data), replace = T, prob = c(0.7,0.3))
trainData = data[ind==1,]
testData = data[ind==2,]
# Building a decision tree
library(party)
myTree = ctree(LEAVE~., data= trainData, controls = ctree_control(mincriterion = 0.99, minsplit =5000 ))
myTree
plot(myTree)
myTree = ctree(LEAVE~., data= trainData, controls = ctree_control(mincriterion = 0.99, minsplit =500 ))
myTree
plot(myTree)
# changing the restrictions
myTree = ctree(LEAVE~., data= trainData, controls = ctree_control(mincriterion = 0.99, minsplit =5000 ))
myTree
plot(myTree)
# prediction
predict(myTree, data= trainData, type = 'prob')
# obtaining the error
err = table(predict(myTree), trainData$LEAVE)
err
1-sum(diag(err))/sum(err)
predData = predict(myTree, newdata = testData)
err1 = table(predData, testData$LEAVE)
ddt = select(data, LEAVE, HOUSE, OVERAGE)
ddt$LEAVE = as.factor(ddt$LEAVE)
ddt$HOUSE= as.numeric(ddt$HOUSE)
ddt$OVERAGE = as.numeric(ddt$OVERAGE)
# partitioning into training and testing
set.seed(1234)
ind= sample(2, nrow(ddt), replace = T, prob = c(0.7,0.3))
trainData = ddt[ind==1,]
testData = ddt[ind==2,]
# Building a decision tree
library(party)
myTree = ctree(LEAVE~., data= trainData, controls = ctree_control(mincriterion = 0.99, minsplit =4500 ))
myTree
########################
# prediction
predict(myTree, data= trainData, type = 'prob')
# obtaining the error
err = table(predict(myTree), trainData$LEAVE)
err
1-sum(diag(err))/sum(err)
predData = predict(myTree, newdata = testData)
err1 = table(predData, testData$LEAVE)
err1
myTree = ctree(LEAVE~., data= trainData, controls = ctree_control(mincriterion = 0.99, minsplit =4500 ))
plot(myTree)
predData = predict(myTree, newdata = testData)
err1 = table(predData, testData$LEAVE)
err1
1-sum(diag(err1))/sum(err1)
myTree = ctree(LEAVE~., data= trainData, controls = ctree_control(mincriterion = 0.99, minsplit =4000 ))
myTree
plot(myTree)
########################
# prediction
predict(myTree, data= trainData, type = 'prob')
# obtaining the error
err = table(predict(myTree), trainData$LEAVE)
err
1-sum(diag(err))/sum(err)
predData = predict(myTree, newdata = testData)
err1 = table(predData, testData$LEAVE)
err1
cat('Importing the data\n')
data = read.csv(file.choose(), header = T)
cat('Checking the data set')
head(data)
cat('Checking the structure if the data\n')
str(data)
library(dplyr)
head(data)
data$COLLEGE = as.numeric(data$COLLEGE)
data$INCOME = as.numeric(data$INCOME)
data$OVERAGE = as.numeric(data$OVERAGE)
data$LEFTOVER = as.numeric(data$LEFTOVER)
data$HOUSE= as.numeric(data$HOUSE)
data$HANDSET_PRICE = as.numeric(data$HANDSET_PRICE)
data$OVER_15MINS_CALLS_PER_MONTH = as.numeric(data$OVER_15MINS_CALLS_PER_MONTH)
data$AVERAGE_CALL_DURATION = as.numeric(data$AVERAGE_CALL_DURATION)
data$REPORTED_SATISFACTION = as.numeric(data$REPORTED_SATISFACTION)
data$REPORTED_USAGE_LEVEL = as.numeric(data$REPORTED_USAGE_LEVEL)
data$CONSIDERING_CHANGE_OF_PLAN = as.numeric(data$CONSIDERING_CHANGE_OF_PLAN)
data$LEAVE = data$LEAVE
cat('PArtitioning data into training and testing')
set.seed(1234)
ind= sample(2, nrow(data), replace = T, prob = c(0.7,0.3))
trainData = data[ind==1,]
testData = data[ind==2,]
cat('Building a decision tree')
library(party)
myTree = ctree(LEAVE~., data= trainData, controls = ctree_control(mincriterion = 0.99, minsplit =500 ))
myTree
plot(myTree)
cat('Importing the data\n')
data = read.csv(file.choose(), header = T)
cat('Checking the data set')
head(data)
cat('Checking the structure if the data\n')
str(data)
library(dplyr)
head(data)
cat('Converting the variables to numeric')
data$COLLEGE = as.numeric(data$COLLEGE)
data$INCOME = as.numeric(data$INCOME)
data$OVERAGE = as.numeric(data$OVERAGE)
data$LEFTOVER = as.numeric(data$LEFTOVER)
data$HOUSE= as.numeric(data$HOUSE)
data$HANDSET_PRICE = as.numeric(data$HANDSET_PRICE)
data$OVER_15MINS_CALLS_PER_MONTH = as.numeric(data$OVER_15MINS_CALLS_PER_MONTH)
data$AVERAGE_CALL_DURATION = as.numeric(data$AVERAGE_CALL_DURATION)
data$REPORTED_SATISFACTION = as.numeric(data$REPORTED_SATISFACTION)
data$REPORTED_USAGE_LEVEL = as.numeric(data$REPORTED_USAGE_LEVEL)
data$CONSIDERING_CHANGE_OF_PLAN = as.numeric(data$CONSIDERING_CHANGE_OF_PLAN)
data$LEAVE = data$LEAVE
cat('PArtitioning data into training and testing')
set.seed(1234)
ind= sample(2, nrow(data), replace = T, prob = c(0.7,0.3))
trainData = data[ind==1,]
testData = data[ind==2,]
cat('Building a decision tree')
library(party)
myTree = ctree(LEAVE~., data= trainData, controls = ctree_control(mincriterion = 0.99, minsplit =500 ))
myTree
cat('Plotting the decision Tree')
plot(myTree)
cat('changing the restrictions')
myTree = ctree(LEAVE~., data= trainData, controls = ctree_control(mincriterion = 0.99, minsplit =5000 ))
#myTree
plot(myTree)
# importing the library to be used to subset the data
library(dplyr)
# Converting the data into the required data type
ddt = select(data, LEAVE, HOUSE, OVERAGE)
ddt$LEAVE = as.factor(ddt$LEAVE)
ddt$HOUSE= as.numeric(ddt$HOUSE)
ddt$OVERAGE = as.numeric(ddt$OVERAGE)
# partitioning into training and testing
set.seed(1234)
ind= sample(2, nrow(ddt), replace = T, prob = c(0.7,0.3))
trainData = ddt[ind==1,]
testData = ddt[ind==2,]
# Building a decision tree
library(party)
myTree = ctree(LEAVE~., data= trainData, controls = ctree_control(mincriterion = 0.99, minsplit =4500 ))
myTree
plot(myTree)
?rtf
`3124656_99653747_places` <- read.csv("C:/Users/ADMIN/Desktop/Nerdy Turtlez/923583 - Data Mining/3124656_99653747_places.txt", header=FALSE)
View(`3124656_99653747_places`)
`data <- read.csv("C:/Users/ADMIN/Desktop/Nerdy Turtlez/923583 - Data Mining/3124656_99653747_places.txt", header=FALSE)
head(data)
data <- read.csv("C:/Users/ADMIN/Desktop/Nerdy Turtlez/923583 - Data Mining/3124656_99653747_places.txt", header=FALSE)
data("iris")
data <- read.csv("C:/Users/ADMIN/Desktop/Nerdy Turtlez/923583 - Data Mining/3124656_99653747_places.txt", header=FALSE)
head(data)
str(data)
names(data)= c('Longitude','Latitude')
head(data)
plot(data$Longitude~., data = data)
plot(Longitude~., data = data)
plot(Longitude~., data)
cat('Normalizatiom\n')
m = apply(data,2,mean)
s = apply(data, 2, sd)
z = scale(data,m,s)
distance = dist(z)
distance
print(distance)
# cluster dendograms
hclust(distance)
# cluster dendograms
hc.c =hclust(distance)
plot(hc.c)
hc.a = hclust(distance, method = 'average')
member.c = cutree(hc.c,3)
print(member.c)
setwd('C:\Users\ADMIN\Desktop\Nerdy Turtlez\923583 - Data Mining')
setwd('C:\\Users\\ADMIN\\Desktop\\Nerdy Turtlez\\923583 - Data Mining')
write.csv(member.c, 'clusters.txt')
member.a = cutree(hc.a,3)
member.a
table(member.c, member.a)
table(member.c)
#write.csv(member.c, 'clusters.txt')
View(member.a)
results = kmeans(data, 3)
results
table(results)
table(results$cluster)
View(results$cluster)
data= read.csv(file.choose(), header = T)
View(data)
dim(data)
data= read.csv(file.choose(), header = F)
dim(data)
# conducting the kmeans algorithm
k.algo = kmeans(data, 3)
k.algo
# conducting the kmeans algorithm
k.algo = kmeans(data, 3)
k.algo
names(data)= c('Longitude','Latitude')
head(data)
m_ean = apply(data,2,mean)
s_td = apply(data, 2, sd)
z = scale(data,m_ean,s_td)
distance = dist(z)
distance
# cluster dendograms
hc.c =hclust(distance)
plot(hc.c)
hc.a = hclust(distance, method = 'average')
member.c = cutree(hc.c,3)
member.a = cutree(hc.a,3)
member.a
cat('Normalizatiom\n')
m_ean = apply(data,2,mean)
s_td = apply(data, 2, sd)
z = scale(data,m_ean,s_td)
distance = dist(z)
# k means algorithm
k.algo =hclust(distance)
results = cutree(k.algo,3)
results
cat('Reading the data set')
data= read.csv(file.choose(), header = F)
str(data)
# naming the columns of the data set
names(data)= c('Longitude','Latitude')
# checking the data set
head(data)
cat('Normalizatiom\n')
# determing the mean
m_ean = apply(data,2,mean)
# determining the standard deviation
s_td = apply(data, 2, sd)
z = scale(data,m_ean,s_td)
distance = dist(z)
# k means algorithm
k.algo =hclust(distance)
results = cutree(k.algo,3)
results
data= read.csv(file.choose(), header = F)
str(data)
# naming the columns of the data set
names(data)= c('Longitude','Latitude')
# checking the data set
head(data)
cat('Normalizatiom\n')
# determing the mean
m_ean = apply(data,2,mean)
# determining the standard deviation
s_td = apply(data, 2, sd)
z = scale(data,m_ean,s_td)
distance = dist(z)
# k means algorithm
k.algo =hclust(distance)
results = cutree(k.algo,3)
results
str(results)
library(iris)
data('iris')
head(iris)
table(iris$Species)
# prediction
predict(myTree, data= training, type = 'prob')
# prediction
predict(myTree, data= training, type = 'prob')
# prediction
pred=predict(myTree, data= training, type = 'prob')
# obtaining the error
err = table(predict(myTree), training$y)
#viewing the top 5 rows in the data set
head(data)
#improting the data set
data = read.csv(file.choose(), header = T, sep = ",")
#viewing the top 5 rows in the data set
head(data)
#******************Data Preprocessing******************#
# checking for missing values
colSums(is.na(data))
# dealing with categorical data
str(data)
# removing unecessry columns
data = data[c(-10,-11)]
# splitting data into training and testing
library(caret)
set.seed(2234)
ind <- createDataPartition(y = data$y, p= 0.7, list = FALSE)
training <- data[ind,]
testing <- data[-ind,]
#checking the dimensions
dim(training); dim(testing);
# building a decision tree by defaul setting
library(party)
mytree= ctree(y~., data= training)
mytree
plot(mytree)
# building a decision tree by 99 % MINIMAL CRITERION and minimal split of 6000
mytree= ctree(y~., data= training,controls = ctree_control(mincriterion = 0.999,minsplit = 6000))
mytree
plot(mytree)
# building a decision tree by 99 % MINIMAL CRITERION and minimal split of 12000
mytree= ctree(y~., data= training,controls = ctree_control(mincriterion = 0.999,minsplit = 12000))
mytree
plot(mytree)
# prediction
pred=predict(myTree, data= training, type = 'prob')
# obtaining the error
err = table(predict(myTree), training$y)
# prediction
predict(myTree, data= training, type = 'prob')
# obtaining the error
err = table(predict(myTree), training$y)
predData = predict(myTree, newdata = testing)
# prediction
predict(myTree, data= training, type = 'prob')
# obtaining the error
err = table(predict(myTree), training$y)
# obtaining the error
err = table(predict(mytree), training$y)
err
1-sum(diag(err))/sum(err)
predData = predict(mytree, newdata = testing)
err1 = table(predData, testing$y)
err1
1-sum(diag(err1))/sum(err1)
sum(diag(err1))/sum(err1)
# prediction
predict(myTree, data= training, type = 'prob')
# obtaining the error
err = table(predict(mytree), training$y)
err
sum(diag(err))/sum(err)
predData = predict(mytree, newdata = testing)
err1 = table(predData, testing$y)
err1
sum(diag(err1))/sum(err1)
# prediction
pred=predict(myTree, data= training, type = 'prob')
# obtaining the error
err = table(predict(mytree), training$y)
err
sum(diag(err))/sum(err)
predData = predict(mytree, newdata = testing)
err1 = table(predData, testing$y)
err1
sum(diag(err1))/sum(err1)
# prediction
pred=predict(myTree, data= training, type = 'prob')
# obtaining the accuaracy for the training data
err = table(predict(mytree), training$y)
err
sum(diag(err))/sum(err)
# determing the accuracy of the testing data
predData = predict(mytree, newdata = testing)
err1 = table(predData, testing$y)
err1
sum(diag(err1))/sum(err1)
# partionin of the data
set.seed(2234)
ind <- createDataPartition(y = data$y, p= 0.7, list = FALSE)
training <- data[ind,]
testing <- data[-ind,]
install.packages("randomForest")
# creating Random FOrest
library(randomForest)
randomForest(y~., data = training)
R.forest = randomForest(y~., data = training)
print(R.forest)
attributes(R.forest)
R.forest$confusion
# preccition and confusion matrix  train data
p1= predict(R.forest, train())
# preccition and confusion matrix  train data
p1= predict(R.forest, training
# preccition and confusion matrix  train data
p1= predict(R.forest, training)
# preccition and confusion matrix  train data
p1= predict(R.forest, training)
p1
head(p1)
head(training$y)
confusionMatrix(p1, training$y)
# prediction of test data
p2= predict(R.forest, testing)
p2
confusionMatrix(p2, testing$y)
# error rate of Random forest
plot(R.forest)
# tuning mtry
tunet=tuneRF(training[,-17], training[,17],
stepFactor = 1,
plot = T,
ntreeTry = 100,
trace = T,
improve = 0.05)
dim(data)
# tuning mtry
tunet=tuneRF(training[,-15], training[,15],
stepFactor = 1,
plot = T,
ntreeTry = 100,
trace = T,
improve = 0.05)
# tuning mtry
tunet=tuneRF(training[,-15], training[,15],
stepFactor = 0.5,
plot = T,
ntreeTry = 100,
trace = T,
improve = 0.05)
# applying the tuned mtry to the model
R.forest = randomForest(y~., data = training,
ntree=100,
mtry=3,
importance=T,
proximity=T)
# applying the tuned mtry to the model
R.forest = randomForest(y~., data = training,
ntree=100,
mtry=3
)
# preccition and confusion matrix  train data
p1= predict(R.forest, training)
confusionMatrix(p1, training$y)
# prediction of test data
p2= predict(R.forest, testing)
confusionMatrix(p2, testing$y)
# no of nodes for the tree
hist(treesize(R.forest), main = 'Random forest',col = 'green')
# applying the tuned mtry to the model
R.forest = randomForest(y~., data = training,
ntree=2200,
mtry=3
)
# varaible importance
varImpPlot(R.forest)
importance(R.forest)
#improting the data set
data = read.csv(file.choose(), header = T, sep = ",")
#viewing the top 5 rows in the data set
head(data)
#improting the data set
data = read.csv(file.choose(), header = T, sep = ",")
#viewing the top 5 rows in the data set
head(data)
# partionin of the data
set.seed(2234)
ind <- createDataPartition(y = data$y, p= 0.7, list = FALSE)
training <- data[ind,]
testing <- data[-ind,]
dim(training); dim(testing)
# creating Random FOrest
library(randomForest)
R.forest = randomForest(y~., data = training)
print(R.forest)
# predition and confusion matrix  train data
p1= predict(R.forest, training)
confusionMatrix(p1, training$y)
# prediction of test data
p2= predict(R.forest, testing)
confusionMatrix(p2, testing$y)
p2= predict(R.forest, testing)
confusionMatrix(p2, testing$y)
# predition and confusion matrix  train data
p1= predict(R.forest, training)
confusionMatrix(p1, training$y)
p1= predict(R.forest, training)
confusionMatrix(p1, training$y)
# prediction of test data
p2= predict(R.forest, testing)
confusionMatrix(p2, testing$y)
confusionMatrix(p2, testing$y)
# error rate of Random forest
plot(R.forest)
# tuning mtry
tunet=tuneRF(training[,-15], training[,15],
stepFactor = 0.5,
plot = T,
ntreeTry = 50,
trace = T,
improve = 0.05)
# applying the tuned mtry to the model
R.forest = randomForest(y~., data = training,
ntree=50,
mtry=2
)
# preccition and confusion matrix  train data
p1= predict(R.forest, training)
confusionMatrix(p1, training$y)
p2= predict(R.forest, testing)
#p2
confusionMatrix(p2, testing$y)
# no of nodes for the tree
hist(treesize(R.forest), main = 'Random forest',col = 'blue')
# no of nodes for the tree
hist(treesize(R.forest), main = 'Distribution of tree in Random forest',col = 'blue')
# varaible importance
varImpPlot(R.forest)
